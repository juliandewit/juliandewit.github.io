---
layout:     post
title:      "2rd place solution for the 2017 national datascience bowl"
date:       2017-04-14 00:00:00
author:     "Julian de Wit"
---


## Summary
This document describes my part of the 2rd prize solution to the [Data Science Bowl 2017](https://www.kaggle.com/c/data-science-bowl-2017/) hosted by Kaggle.com. I teamed up with Daniel Hammack. His part of the solution is decribed [here](https://www.google.com/) The goal of the challenge was to predict the development of lung cancer in a patient given a set of CT images. Detailed descriptions of the challenge can be found on the [Kaggle competition page](https://www.kaggle.com/c/data-science-bowl-2017#description) and [this blog post](https://eliasvansteenkiste.github.io/machine%20learning/lung-cancer-pred/) by Elias Vansteenkiste. My solution (and that of Daniel) was mainly based on nodule detectors with a 3D convolutional neural network architecture.


## Initial familiarization and the resulting "plan of attack"
Before joining the competition I first watched the [video by Bram van Ginneken on lung CT images](https://www.youtube.com/watch?v=-XUKq3B4sdw) to get a feel for the problem. I tried to manually asses a few scans and concluded that this was a hard problem where you almost literally had to find a needle in a haystack. Like described by [Elias Vansteenkiste](https://eliasvansteenkiste.github.io/machine%20learning/lung-cancer-pred/) the amount of signal vs noise was almost 1:1000.000. As the first efforts on the forums showed, the neural nets were not able to learn someting from the raw image data. There were only 1300 cases to train on and the label "Cancer Y/N" was to distant from the actual features in the images for the network to latch upon. 

The solution would be to spoonfeed a neural network with examples with a better signal/noise ratio and a more direct relation between the labels and the features. Luckily the competition organizers already pointed us to a previous competition called [LUNA16](https://luna16.grand-challenge.org/). For this dataset doctors had meticulously labeled more than 1000 lung nodules in more than 800 patient scans. The LUNA16 competition also provided non-nodule annotations. So when you crop small 3D chunks around the annotations from the big CT scans you end up with much smaller 3D images with a more direct connection to the labels (nodule Y/N). The dataset also contained size information. As the size usually is a good predictor of being a cancer so I thought this would be a useful starting point.

![The plan](/images/plan2017_2.png)
*Figure 1. High level description of the approach*

Later I noticed that the LUNA16 dataset was drawn from another public dataset [LIDC-IDRI](https://wiki.cancerimagingarchive.net/display/Public/LIDC-IDRI). It turned out that in this original set the nodules had not only been detected by the doctors but they also gave an assessment on the malignancy and other properties of the nodules. This malignancy assessment turned out to be learnable by the neural network and a "golden" feature for estimating the cancer risk.

The final plan of attack was to train a neural network to detect nodules and predict the malignancy of the detected nodules. During prediction every patient scan would be processed by the network going over it in a sliding window fashion. This resulted in a lattice containing malignancy information for every location that the sliding window had visited. The final step was to predict the chance that that the patient would develop a cancer given this information and some other features.


## Preprocessing and creating a trainset
The preprocessing of the CT scans was the first step. It was important to make the scans as homogenous as possible. To do this, first every scan was rescaled so that every voxel represented an areas of 1x1x1 mm. Both Daniel and me did some experiments with other scales but 1mm was a good balance between accuracy and computational load. With CT scans the pixel intensities can be expressed in [Hounsfield Units](https://en.wikipedia.org/wiki/Hounsfield_scale) and have semantic meaning. As suggested on the forums all intensities were clipped on the min, max interesting hounsfield value and then scaled between 0 and 1. The last importand CT preprocessing step was to make sure that all scans had the same orientation. 

Almost all the literature on nodule detection and almost all tutorials on the forums advised to first segment out the lung tissue from the CT-scans. However, none of the segmentation approaches were good enough to adequately handle nodules and masses that were hidden near the edges of the lung tissue. Sometimes these were removed from the images leaving no chance for the nodule detector to find. I first considered training a U-net to properly segment the lungs. This would almost surely give better results than traditional segmentation techniques. However, when I visually inspected CT scans, the borders of the lung tissue gave me a good frame of reference to find nodules. It was my hunch that the convnet might also "like" this information. So in the end I concluded that segmenting the lungs was a pre-convnet relic from the past and decided to train and predict on raw images.

In more straight forward competition the traindata is a given and is not interesting to discuss. However, for this solution engineering trainset was an essential, if not the most essential part. I used provided labels, generated automatic labels, employed automatic active learning and also added some manual annotations. Below is a table with the different sources I used as labels.


![Labelsets](/images/table_labels.png)
*Table 1. Labelsets used for training.*

The quantity of positive doctor labels from LIDC is five times the number of the LUNA16 set. The reason is that these are the combined annotations of 4 doctors. So one nodule can be annotated 4 times. LUNA16 also ignored nodules that were only annotated by less than 3 doctors. I decided to keep these in because of the valuable malignancy information that was provided with these labels. 

The candidates(v2) set were taken straight from LUNA16. These were false positive candidate nodules taken from a wide range nodule detection systems. Note that some of these candidates overlapped nodules that were tagged by less than 3 doctors. I kept them in to provide some counter balance. To train on the full images I needed negative candidates from non-lung tissue. I used a simple lung segmentation algorithm from the forums and samples annotations around the edges of the segmentation masks. After doing a first training round I predicted nodules on the LUNA16 datasets. All false positives were harvested and added to the trainset. 

After I finished building my model I wanted to build another model. For my second model I played radiologist and let the network predict on the NDSB trainset. Then I manually tried to select interesting positive nodules from cancer cases and false positives from non-cancer cases. Then I trained a second model with these extra labels. I expected better results but it turned out that I am a bad radiologist since the second model with my manual labels was worse then the model without. However, the blend of the two models was better than the seperate models so I kept the second model in. 

I used a special, hastily built, viewer to debug all the labels. While viewing I noticed that some >3cm big nodules were ignored by the doctors. Reading the LIDC documentation I found that the doctors were ordered to ignore >3m labels. Fearing that my classifier would be confused by these ignored masses I removed negatives that overlapped with them. Below are some screenshot I took.

![Labelscreenshots](/images/label_examples.png)

*Figure 2. Label visualizations. Top left: Luna16 candidates, Top right: Non lung tissue edge, Bottom left: A false positive, Bottom right: Non annotated mass I removed candidates around.*


## 3D convnet training strategy and architecture
Even with a better trainset it still considable effort to effectively train a neural network. The dataset was still heavily imbalanced (5000:500000) and there was much variation in size and shape of the positive examples. I had consisered U-net architectures but 2D U-nets could not exploit the inherently 3D structure of the nodules and 3D U-nets were quite slow and inflexible. The main decision to skip U-nets was that it was not necessary to have a fine-grained probability map but just a coarse detector. Having a small 3D convnet that you slide over the CT scans was much more lightweight and flexible.

My first goal was to train a working nodule predictor. The first thing I did was to upsample the positive examples to a ratio of 1:20. For generalization I tried a number of augmentation strategies but somehow only loss-less augmentations helped. In the end I used heavy translations and all 3D flips. 

Once the classifier was in place I wanted to train a malignancy estimator. The provided malignancy labels ranged from 1 (very likely not malignant) to 5 (very likely malignant). To put more weight on the malignant examples I squared the labels to range from 1 to 25. At first I was thinking about the 2 stage approach where first nodules were classified and then another network would be trained on the nodule for malignancy. To win time I tried one network to train both at once in a multi-task learning approach. Somehow this worked quite well and since the approach was quick and simple I decided to go fo this.

Usually the architecture of the neural network is one of the most important outcomes of a competition or case study. For this competition I spent relatively little time on the neural network architecture. I think this is mainly that there are already so many good baseline architectures. I started out with some simple VGG and resnet-like architectures. All performed roughly the same. Then I wanted to try a pretrained [C3D](http://vlg.cs.dartmouth.edu/c3d/) network. The pretrained weights did not help at all but the architecture without pretrained weights gave a very good performance. The final architecture was basically C3D with a few adjustments.

The adjustments were the receptive field which I set to 32x32x32 mm. This might sound like a bit too small but it worked very good with some tricks later in the pipeline. The idea was keep everything lightweight and make a bigger net on the end of the competition. But since Daniel's network was 64x64x64 mm I decided to stay at the small receptive field so that we were as complimentary as possible. The second adjustment I made was to immediately average pool the z-axis to 2mm per voxel. This made the net much lighter and did not effect accurracy since for most scan the z-axis was at a more coarse scale than the x and y axes. Finally I introduced a 64 unit bottleneck layer on the end of the network. The idea was to do some experiments with training on the raw intermediate features instead of the predicted malignancy later in the process.


![Network architecture](/images/network_table.png)

*Table 2. Network architecture.*


## Final step cancer prediction
Once the network was trained the next was to let the neural network detect nodules and estimate their malignancy. Given this data and some extra features I wanted to train a gradient boosting classifier to predict the development of cancer within one year. All this was relatively straight forward. The main problem was that the leaderboard was based on 200 patients and contained, by accident, a big number of outlier patients. After some tweaking my local cross validation was 





## Spicing things up: strange tissue detector

## Teaming up with Daniel Hammack

## Conclusion and remarks


## Code
The code can be found [here](https://github.com/juliandewit/kaggle_ndsb2)	

## Thanks	
1. *Kaggle and Booz Allen Hamilton.*<br> Thank you for organizing this complex and cool challenge. There were some complaints about the whole two phase setup and the quality of the data. But if we want to take on more ambitious problems than the usual CTR/forecasting stuff you sometimes need to try something new an take some risk.

2. *Mxnet.*<br> What can I say... great library when you also want to deploy your systems in real-world situations. Especially good windows support is something that is severely lacking from most other libraries.

3. *The authors of the U-net paper.* <br> The idea was great. The paper was easy to understand with clear language and concrete examples. That is something you do not find everyday in the deep learning community.




  
  

|Layer|Params|Activation|Output|Remark|
|---|:-:|:-:|--:|---|
|Input|   |   |32x32x32,1||
|Avg pool|2x1x1|   |16x32x32,1|Downsample z-axis|
|3D conv|3x3x3|relu|16x32x32,64||
|Max pool|1x2x2|   |16x16x16,64|Axes are same again|
|3D conv|3x3x3|relu|16x16x16,128||
|Max pool|2x2x2|   |8x8x8,128||
|3D conv (2x)|3x3x3|relu|8x8x8,256||
|Max pool|2x2x2|   |4x4x4,256||
|3D conv (2x)|3x3x3|relu|4x4x4,512||
|Max pool|2x2x2|   |2x2x2,512||
|3D conv|2x2x2|relu|1x1x1, 64|Bottleneck features|
|3D conv|2x2x2|sigmoid|1x1x1, 1|Nodule detector|
|3D conv|2x2x2|none|1x1x1, 1|Malignancy estimator|














  
